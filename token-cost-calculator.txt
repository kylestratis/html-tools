===========================================
AI Token Cost Calculator - Conversation Log
===========================================

Date: December 18, 2024
Generated by: Claude (Anthropic)
Session: html-tools project

---

ORIGINAL REQUEST:
-----------------
User requested a tool that:
1. Takes text input (treated as AI prompts)
2. Has a dropdown with all major AI models
3. Estimates token count for the input text
4. Calculates the cost based on input token pricing
5. Can use an external calculator if needed

Additionally, user requested updating CLAUDE.md to document that:
- For every conversation that results in a new tool
- A text file with the same name as the HTML file should be saved
- The text file should contain the conversation that generated the tool

---

IMPLEMENTATION DECISIONS:
--------------------------

1. Token Counting Method:
   - Decided to use a simple 4:1 character-to-token ratio
   - This is a standard approximation for English text
   - Avoids external dependencies (keeping with project philosophy)
   - More accurate than word-based counting
   - Note added to UI explaining this is an estimation

2. AI Models Included:
   OpenAI:
   - GPT-4o: $2.50/$10.00 per 1M tokens
   - GPT-4o-mini: $0.15/$0.60 per 1M tokens
   - GPT-4 Turbo: $10.00/$30.00 per 1M tokens
   - GPT-4: $30.00/$60.00 per 1M tokens
   - GPT-3.5 Turbo: $0.50/$1.50 per 1M tokens

   Anthropic Claude:
   - Claude Opus 4: $15.00/$75.00 per 1M tokens
   - Claude Sonnet 4: $3.00/$15.00 per 1M tokens
   - Claude 3.5 Sonnet: $3.00/$15.00 per 1M tokens
   - Claude 3 Opus: $15.00/$75.00 per 1M tokens
   - Claude 3 Haiku: $0.25/$1.25 per 1M tokens

   Google Gemini:
   - Gemini 1.5 Pro: $1.25/$5.00 per 1M tokens
   - Gemini 1.5 Flash: $0.075/$0.30 per 1M tokens
   - Gemini 2.0 Flash: $0.10/$0.40 per 1M tokens

   Meta Llama:
   - Llama 3.3 70B: $0.35/$0.40 per 1M tokens
   - Llama 3.1 405B: $0.80/$0.80 per 1M tokens

   Mistral:
   - Mistral Large: $2.00/$6.00 per 1M tokens
   - Mistral Small: $0.20/$0.60 per 1M tokens

   Pricing current as of December 2024

3. Features Implemented:
   - Real-time character counting
   - Real-time token estimation
   - Cost calculation for input tokens
   - Display of both input and output pricing (for reference)
   - Model-organized dropdown with optgroups
   - Dark mode support (required by project standards)
   - Responsive design
   - Clear information about estimation method

4. UI Design:
   - Large textarea for prompt input
   - Clear dropdown for model selection
   - Results panel showing:
     * Character count
     * Estimated tokens
     * Estimated cost (prominently displayed)
     * Model pricing details (input/output per 1M tokens)
   - Informational note about estimation accuracy
   - Follows project color scheme (purple gradient)
   - Dark mode toggle in top right

5. Cost Display Format:
   - Very small costs (< $0.00001): Scientific notation
   - Small costs (< $0.01): 5 decimal places
   - Medium costs (< $1): 4 decimal places
   - Larger costs: 2 decimal places
   - This ensures precision for small values while being readable

---

TECHNICAL IMPLEMENTATION:
--------------------------

File: token-cost-calculator.html

Structure:
- Single HTML file with inline CSS and JavaScript
- No external dependencies
- All processing client-side
- Follows project dark mode standards

Key Functions:
- estimateTokens(text): Calculates tokens using 4:1 ratio
- calculateCost(tokens, modelKey): Computes cost from pricing table
- formatCost(cost): Formats currency with appropriate precision
- updateCalculations(): Main function that recalculates on input/change
- Dark mode functions: getPreferredTheme(), setTheme(), toggleTheme()

Data Structure:
- modelPricing object: Maps model IDs to pricing and display names
- Contains input/output pricing per 1M tokens for all models

---

ADDITIONAL CHANGES:
-------------------

1. Updated index.html:
   - Added tool to the Available Tools list
   - Description: "Estimate token counts and costs for AI model prompts"
   - Features listed: Character & token counting, 15+ AI models, real-time estimates, input token pricing

2. Updated README.md:
   - Added tool documentation under Available Tools section
   - Listed key features and capabilities

3. Updated .claude/CLAUDE.md:
   - Added new section "5. Document the Conversation (AI-Generated Tools Only)"
   - Explained when and how to create conversation text files
   - Updated File Structure section to show optional .txt files
   - Documented the purpose and contents of conversation files

---

CHALLENGES & NOTES:
-------------------

1. Token Estimation:
   - Considered using tiktoken library via CDN but decided against it
   - Keeps tool simple and dependency-free
   - 4:1 ratio is industry-standard approximation
   - Actual counts vary by tokenizer but estimation is close enough for cost planning

2. Pricing Data:
   - Hardcoded pricing as of December 2024
   - May need updates as providers change pricing
   - Could consider adding a "last updated" note in the future
   - Focused on input pricing since tool is for prompt estimation

3. Dark Mode:
   - Implemented according to project standards
   - Uses CSS variables for all colors
   - localStorage persistence
   - System preference detection
   - Smooth transitions

---

FUTURE ENHANCEMENTS (Not Implemented):
---------------------------------------

Possible future additions:
- Export results to CSV or JSON
- Compare multiple models side-by-side
- Output token estimation (for expected response length)
- Batch processing for multiple prompts
- Historical pricing data/trends
- API integration for exact token counts
- Support for more models as they're released

However, current implementation follows project philosophy of simplicity.

---

FILES CREATED/MODIFIED:
-----------------------

Created:
- token-cost-calculator.html (the tool)
- token-cost-calculator.txt (this file)

Modified:
- index.html (added tool to listing)
- README.md (added tool documentation)
- .claude/CLAUDE.md (added conversation documentation requirement)

---

MAJOR UPDATE - December 18, 2024
=================================

ENHANCEMENT REQUEST:
--------------------
User requested:
1. Integration with js-tiktoken library from CDN for accurate OpenAI token counting
2. Integration with Anthropic's count_tokens API endpoint (v1/messages/count_tokens)
3. Optional API key management for Anthropic API (stored in localStorage)
4. Clear privacy messaging about local storage
5. Fallback to estimates when API key is not provided
6. More prominent estimates disclaimer

RESEARCH FINDINGS:
------------------

1. Pricing APIs:
   - No providers (OpenAI, Anthropic, Google, Meta, Mistral) offer pricing via API
   - Pricing must remain hardcoded and manually updated
   - Feature requests exist but not implemented

2. Token Counting APIs:
   - OpenAI: No API, but TikToken library available (js-tiktoken via CDN)
   - Anthropic: Has free token counting API at v1/messages/count_tokens
     * Requires API key
     * Beta header: "token-counting-2024-11-01"
     * Free to use (subject to rate limits)
     * Returns exact token counts

3. Selected Libraries:
   - js-tiktoken v1.0.14 from jsDelivr CDN (~50-100KB)
   - Pure JavaScript implementation
   - Works perfectly client-side
   - Supports o200k_base (GPT-4o) and cl100k_base (GPT-4, GPT-3.5) encodings

IMPLEMENTATION DETAILS:
-----------------------

1. External Dependencies Added:
   - js-tiktoken v1.0.14 via ES Module import from CDN
   - Uses modern ES6 module syntax (type="module")
   - Loads encodings on-demand

2. API Key Management:
   - Settings modal with gear icon (⚙️) button
   - Password input for Anthropic API key
   - localStorage key: 'anthropic_api_key'
   - Clear privacy messaging about localStorage-only storage
   - Save/Clear/Cancel buttons
   - Modal can be closed by clicking outside

3. Token Counting Logic:
   Three methods based on model type:

   a) OpenAI Models (TikToken):
      - Uses js-tiktoken library
      - Accurate token counts
      - No API key needed
      - Encodings: o200k_base, cl100k_base
      - Caches encoders for performance

   b) Claude Models (Anthropic API):
      - Calls v1/messages/count_tokens endpoint
      - Requires API key (optional)
      - Falls back to estimates if no key
      - Maps UI model names to API model IDs
      - Handles 401 errors (invalid key)
      - Shows status messages

   c) Other Models (Estimates):
      - Gemini, Llama, Mistral
      - Uses 4:1 character-to-token ratio
      - No tokenizer available

4. UI Enhancements:
   - Settings button next to dark mode toggle
   - Token count shows method indicator: "(TikToken)", "(Anthropic API)", or "(Estimated)"
   - Status message system with success/warning/info styles
   - Enhanced info box explaining counting methods
   - Modal overlay with dark mode support

5. Model Mapping:
   Added apiModel field to modelPricing object:
   - claude-opus-4 → "claude-opus-4"
   - claude-sonnet-4 → "claude-sonnet-4"
   - claude-3.5-sonnet → "claude-3-5-sonnet-20241022"
   - claude-3-opus → "claude-3-opus-20240229"
   - claude-3-haiku → "claude-3-haiku-20240307"

6. Error Handling:
   - TikToken errors fall back to estimates
   - Anthropic API errors fall back to estimates
   - Invalid API key shows warning and clears from storage
   - Network errors logged to console
   - Graceful degradation throughout

7. Privacy & Security:
   - API key stored only in localStorage
   - Never transmitted except to Anthropic's official API
   - Clear privacy notice in settings modal
   - Password input type for API key
   - Autocomplete disabled

NEW TECHNICAL FUNCTIONS:
------------------------

- getEncoder(encoding): Creates/retrieves cached TikToken encoders
- countTokensWithTiktoken(text, encoding): Accurate OpenAI token counting
- countTokensWithAnthropic(text, modelId): Calls Anthropic API
- showStatus(message, type): Displays temporary status messages
- Modal event handlers for settings management

UPDATED UI ELEMENTS:
--------------------

- Settings button (top right, left of dark mode toggle)
- Settings modal with API key input
- Token method indicator next to token count
- Status message container for feedback
- Enhanced info box with method explanations
- Warning/success/info message styles (dark mode compatible)

CSS ADDITIONS:
--------------

- Modal overlay and content styles
- Button styles (primary, secondary, danger)
- Warning/success message colors
- Settings button positioning
- Token method indicator styling
- Mobile-responsive modal

SOURCES CONSULTED:
------------------

- Anthropic Claude API Documentation: https://docs.claude.com/en/api/messages-count-tokens
- js-tiktoken CDN: https://www.jsdelivr.com/package/npm/js-tiktoken
- OpenAI Pricing Endpoint Discussion: https://community.openai.com/t/...
- AI Model Pricing Comparisons: https://api.chat/models/

TESTING SCENARIOS:
------------------

1. OpenAI models: Accurate counts via TikToken (no API key needed)
2. Claude models without API key: Falls back to estimates
3. Claude models with valid API key: Exact counts via API
4. Claude models with invalid API key: Warning shown, falls back to estimates
5. Other models: Always use estimates
6. Dark mode: All new UI elements support dark mode
7. Mobile: Settings button repositions, modal is responsive

FILES MODIFIED:
---------------

- token-cost-calculator.html (complete rewrite with new features)
- token-cost-calculator.txt (this file, documenting the enhancement)

BREAKING CHANGES:
-----------------

None - tool is backward compatible. New features are additive.

DEPENDENCY ADDED:
-----------------

- js-tiktoken@1.0.14 from https://cdn.jsdelivr.net/npm/js-tiktoken@1.0.14/+esm
- ~100KB additional load for accurate OpenAI token counting
- Acceptable per project guidelines for lightweight external dependencies

---

END OF CONVERSATION LOG
